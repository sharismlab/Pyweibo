#!/usr/bin/env python
# -*- coding: utf-8 -*-

from argparse import ArgumentParser
import datetime
import Pyweibo

# TEST URLs
# short = 'http://e.weibo.com/1930665641/zeVxsoiyB'
# huge = 'http://www.weibo.com/1701401324/zeoBquVKi'

# python src/main.py http://e.weibo.com/1930665641/zeVxsoiyB -a map


def main():
    desc ="PyWeibo is a crawler and visualization tool for Sina Weibo"
    usage="""
        pyweibo URL [-a 'action'] [options] 
        """
    parser = ArgumentParser(usage=usage, version=" 0.1", description=desc)

    parser.add_argument("URL", help="the URL from Sina Weibo (post or user profile)")
    
    parser.add_argument('-a', "--action", 
                      help='Select a way to process the data', 
                      dest='action', 
                      action='store',
                      metavar='<map/graph/tag/feel>',
                      required=True)

    parser.add_argument("-o", "--outputfile",
                      action="store", # optional because action defaults to "store"
                      dest="outputfile",
                      default="./out/graph",
                      help="Name of the file to write graph info (time tag will be added). If empty, a name is generated by default from URL",
                      metavar='<FILENAME>')

    parser.add_argument("-d", "--database",
                      action="store",
                      dest="db",
                      default=False,
                      help="select between MongoDB and Redis to store raw data")

    parser.add_argument("-g", "--graph",
                      action="store", # optional because action defaults to "store"
                      dest="graphtype",
                      default="dot",
                      help="chose Graph file type: .dot or .gdf (for Gephi). Default: .dot",
                      metavar='<graphtype>')

    parser.add_argument("-l", "--level",
                      dest="level",
                      default="2",
                      help="crawler depth",
                      type=int,
                      metavar='<graphtype>')

    parser.add_argument("-m", "--max",
                      dest="max",
                      default="10000",
                      help="Maximum number of posts to crawl",
                      type=int,
                      metavar='<graphtype>')

    args = parser.parse_args()


    # validate Sina Weibo URL (to do)
    # if args.URL

    # Naming file
    filename = './out/graph' # default
    suffix = datetime.datetime.now().strftime("%y%m%d_%H%M%S")

    if args.outputfile:
      filename = "_".join([args.outputfile, suffix])
    else: 
      # Generate safe name from post URL
      posturl = args.URL
      urlparts = posturl.split('/')
      filename = 'post_%s_%s_%s'%(urlparts[3],urlparts[4],suffix)

    if args.graphtype == "gdf":
      gdffile = "./out/"+filename+".gdf"

    else:
      dotfile = "./out/"+filename+".dot" 

    # Build request
    level=2
    maxposts=100

    if args.max:
      maxposts = args.max
    if args.level:
      level = args.level

    # Storage
    if args.db == False:
        # check if filename 
        print "Data will be written to local file 'out/data' "

    elif args.db == "mongo":
        print "Data will be stored to Mongo"

    elif args.db == "redis":
        print "Data will be stored to Redis"
    
    # Take actions

    pyweibo = Pyweibo.Pyweibo() # init

    if args.action=="map":
        print "You asked for a repost map of %s"%(args.URL)
        
        pyweibo.generateRepostMap(args.URL, level, maxposts)



    elif args.action=="tag":
        print "You asked for a report comments of %s"%(args.URL)

    elif args.action=="map":
        print "You asked for a social graph of profile %s"%(args.URL)

    elif args.action=="feel":
        print "You asked for a sentiment analysis"




    print args

if __name__ == '__main__':
    main()


